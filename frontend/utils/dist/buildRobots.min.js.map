{"version":3,"sources":["buildRobots.js"],"names":["fs","require","path","APP_CONFIG","generateRobotsTxtDuringBuild","baseUrl","robotsTxt","robotsPath","regeneratorRuntime","async","_context","prev","next","process","NEXT_PUBLIC_BASE_URL","BASE_SITE_URL","concat","join","cwd","writeFileSync","console","log","error","stop"],"mappings":"aAAA,IAAMA,GAAKC,QAAQ,MACbC,KAAOD,QAAQ,iBADrBA,QAAA,mBAAQE,oBAAAA,WAKR,SAAeC,+BAAf,IAAAC,EAAAC,EAAAC,EAAA,OAAAC,mBAAAC,MAAA,SAAAC,GAAA,OAAA,OAAAA,EAAAC,KAAAD,EAAAE,MAAA,KAAA,EACI,IACUP,EAAUQ,QALhBV,IAK4BW,sBAHpCX,WAAAY,cAKcT,EAHN,kDAAAU,OADRX,EACQ,sMADRE,EAAAL,KAAAe,KAAAJ,QAAAK,MAAA,SAAA,cAAAlB,GAAAmB,cAAAZ,EAAAD,GAAAc,QAAAC,IAAA,oDAAA,MAAAC,GAAAF,QAAAE,MAAA,8CAAAA,GAAA,KAAA,EAAA,IAAA,MAAA,OAAAZ,EAAAa,UA8BmBnB,OAAAA,QAAAA,CAAAA,6BAAAA","file":"buildRobots.min.js","sourcesContent":["const fs = require('fs');\r\nconst path = require('path');\r\nconst { APP_CONFIG } = require('../constants.js');\r\n\r\n// Funcție pentru generarea robots.txt în timpul build-ului\r\nasync function generateRobotsTxtDuringBuild() {\r\n    try {\r\n        const baseUrl = process.env.NEXT_PUBLIC_BASE_URL || APP_CONFIG.BASE_SITE_URL;\r\n\r\n        const robotsTxt = `User-agent: *\r\nAllow: /\r\n\r\n# Sitemap\r\nSitemap: ${baseUrl}/sitemap.xml\r\n\r\n# Crawl-delay\r\nCrawl-delay: 1\r\n\r\n# Disallow admin areas (if any)\r\nDisallow: /admin/\r\nDisallow: /api/\r\n\r\n# Allow all other content\r\nAllow: /blog/\r\nAllow: /images/\r\nAllow: /assets/`;\r\n\r\n        const robotsPath = path.join(process.cwd(), 'public', 'robots.txt');\r\n        fs.writeFileSync(robotsPath, robotsTxt);\r\n        console.log('✅ Robots.txt generated successfully during build');\r\n    } catch (error) {\r\n        console.error('❌ Error generating robots.txt during build:', error);\r\n    }\r\n}\r\n\r\nmodule.exports = { generateRobotsTxtDuringBuild };\r\n"]}